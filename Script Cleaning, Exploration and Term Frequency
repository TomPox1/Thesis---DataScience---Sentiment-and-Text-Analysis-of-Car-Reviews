library(readxl)
library(dplyr)
library(ggplot2)
library(ngram)
library(udpipe)
library(tm)
library("stopwords")

#Exploring Data and finding a good combination of car model and year in order to have a suitable number of reviews
Nuovo<-read.csv("C:/Users/tomma/Desktop/Codici\\Review.csv")
Nuovo
Nuovo.col<-Nuovo[,2]
elenco<-table(Nuovo.col)
sort(elenco)


#Models reviewed after year 2000
prova <- Nuovo %>% filter(Nuovo[,3] >= 2000)

#Extraction of 4 Models in 2016
Estrazione <- {

prius_2016 <- prova %>% filter(prova[,2] == "prius",prova[,3] == 2016)
focus_2016 <- prova %>% filter(prova[,2] == "focus", prova[,3] == 2016)
highlander_2016 <- prova %>% filter(prova[,2] == "highlander", prova[,3] ==2016)
tucson_2016 <- prova %>% filter(prova[,2] == "tucson", prova[,3] ==2016)

macchine <-list(
  prius_2016,
  focus_2016,
  highlander_2016,
  tucson_2016
  )
}


D_1<-as.data.frame(macchine[1])
D_2<-as.data.frame(macchine[2])
D_3<-as.data.frame(macchine[3])
D_4<-as.data.frame(macchine[4])

D<-rbind(D_1,D_2,D_3,D_4)

{
# Import reviews texts
D_1$testop <- cleanTesto(D_1$Review,hashtag = F,mention = T,numeri = F,punteggiatura = F,minuscolo =T)

D_1$testop <- iconv(x = D_1$testop, to = "UTF-8")

# pos tagging and lemmatization
# loading model
ud_model_EN <- udpipe_load_model("C:/Users/Tommaso/Desktop/Tesinat/Codici\\english-ewt-ud-2.5-191206.udpipe")

# Creating a StopWords list

D_1$doc_id <- 1:nrow(D_1)

c<- c("dealer","year","honda","fiat","car","tucson","highlander","ford","new","manual","automatic","get","focus","prius","accord","toyota","camry","civic","vehicle","rav4")
dfLemm <-lemmaUDP(x = D_1$testop,
                   model = ud_model_EN,
                   doc_id = D_1$doc_id,
                   stopw = tm::stopwords("english"),
                   userstopw = c)


# Reconstruction of lemmatized sentences
txtLem <- dfLemm %>%
  filter(!is.na(lemma) & STOP==FALSE ) %>%
  select(doc_id,lemma) %>%
  group_by(doc_id=as.numeric(doc_id)) %>%
  summarise(txtL=paste(lemma,collapse = " "))

D_1 <- left_join(D_1,txtLem,by="doc_id")

# Compound words correction
mycorrez <- c(
  "steering wheel","steering_wheel",
  "fun to drive", "fun_to_drive",
  
  "brand new","brand_new",
  
  "stop work", "stop_work",
  "gas mileage", "gas_mileage",
  "fuel economy", "fuel_economy",
  
  "driver seat", "driver_seat",
  "touch screen", "touch_screen",
  "front seat", "front_seat",
  "adaptive cruise control", "adaptive_cruise_control",
  "back camera", "back_camera",
  "vibration quality","vibration_quality",
  "blind spot", "blind_spot",
  "safety feature", "safety_feature",
  "cruise control", "cruise_control",
  "navigation system", "navigation_system",
  "climate control", "climate_control",
  "back seat", "back_seat",
  "sound system", "sound_system",
  
  
  "steering wheel", "steering_wheel",
  "mile per gallon", "MPG",
  
  "rear seat", "rear_seat",
  "oil change", "oil_change",
  "leather seat", "leather_seat",
  "front passenger seat", "front_passenger_seat",
  "rear view camera", "rear_view_camera",
  "center console", "center_console",
  "sport mode", "sport_mode",
  "driver side", "driver_side",
  "accord sport", "accord_sport",
  "audio system", "audio_system",
  "sound system", "sound_system",
  "lane departure warning", "lane_departure_warning",
  "side view camera", "side_view_camera",
  "side view  mirror", "side_view_mirror",
  "new civic", "new_civic",
  "passenger side", "passenger_side",
  "seat comfortable", "seat_comfortable",
  "camry special edition", "camry_special_edition",
  "infotainment system", "infotainment_system",
  "leg room", "leg_room",
  "gas mileage", "gas_mileage",
  "test drive", "test_drive",
  "blind spot", "blind_spot",
  "adaptive cruise control","adaptive_cruise_control",
  "driver seat", "driver_seat",
  "cruise control", "cruise_control",
  "back seat", "back_seat",
  "front seat", "front_seat",
  "fuel economy", "fuel_economy",
  "test drove", "test_drive",
  "pretty good", "pretty_good",
  "apple carplay", "appl_carplay",
  "2 years", "2_years",
  "3 years", "3_years",
  "touring model", "touring_model",
"prius", "",
  "cr v", "",
"camry", " ",
"accord", " ",
"crv", " ",
"civic", " ",
"rav4", " ",
"honda", " ",
"car", " ",
"vehicle", " ",
"toyota", " ",
"fiat", " ",
"feels like", "feels_like",
"feel like", "feel_like",
"around town", "around_town",
"prius", " ",
  "ford"," ",
"get"," ",
"comment"," ",
"process", " "
  
  
)
D_1$txtL.y <- corFrmComp(vText = D_2$txtL.y,correzioni = mycorrez)


D_1<- D%>% filter(Model=="prius")
D_2<- D%>% filter(Model=="focus")
D_3<- D%>% filter(Model=="highlander")
D_4<- D%>% filter(Model=="tucson")

# Vectorization and TF weighting for all 4 brands. 
# VectorSource argument was changed for every brand.
SLcorpus <- Corpus( VectorSource(D_1$txtL) )
SLcorpus

# TF weighting
SLtdm <- TermDocumentMatrix(SLcorpus)
mSLtdm <- as.matrix(SLtdm)
dfSLtdm <- data.frame(words=rownames(mSLtdm),mSLtdm,freq=rowSums(mSLtdm))
rownames(dfSLtdm) <- NULL
freqTF <- dfSLtdm %>%
  select(words,freq) %>%
  top_n(n = 20,wt = freq) %>%
  arrange(-freq)

freqTF

SLtdmIDF <- weightTfIdf(SLtdm)
mSLtdmIDF <- as.matrix(SLtdmIDF)
dfSLtdmIDF <- data.frame(words=rownames(mSLtdmIDF),mSLtdmIDF,freq=rowSums(mSLtdmIDF))
rownames(dfSLtdmIDF) <- NULL
prius_freqIDF <- dfSLtdmIDF %>%
  select(words,freq) %>%
  top_n(20,freq) %>%
  arrange(-freq)
prius_freqIDF

SLcorpus <- Corpus( VectorSource(D_2$txtL) )
SLcorpus

SLtdm <- TermDocumentMatrix(SLcorpus)
mSLtdm <- as.matrix(SLtdm)
dfSLtdm <- data.frame(words=rownames(mSLtdm),mSLtdm,freq=rowSums(mSLtdm))
rownames(dfSLtdm) <- NULL
freqTF <- dfSLtdm %>%
  select(words,freq) %>%
  top_n(n = 20,wt = freq) %>%
  arrange(-freq)

freqTF

SLtdmIDF <- weightTfIdf(SLtdm)
mSLtdmIDF <- as.matrix(SLtdmIDF)
dfSLtdmIDF <- data.frame(words=rownames(mSLtdmIDF),mSLtdmIDF,freq=rowSums(mSLtdmIDF))
rownames(dfSLtdmIDF) <- NULL
focus_freqIDF <- dfSLtdmIDF %>%
  select(words,freq) %>%
  top_n(20,freq) %>%
  arrange(-freq)
focus_freqIDF

SLcorpus <- Corpus( VectorSource(D_3$txtL) )
SLcorpus


SLtdm <- TermDocumentMatrix(SLcorpus)
mSLtdm <- as.matrix(SLtdm)
dfSLtdm <- data.frame(words=rownames(mSLtdm),mSLtdm,freq=rowSums(mSLtdm))
rownames(dfSLtdm) <- NULL
freqTF <- dfSLtdm %>%
  select(words,freq) %>%
  top_n(n = 20,wt = freq) %>%
  arrange(-freq)

freqTF

SLtdmIDF <- weightTfIdf(SLtdm)
mSLtdmIDF <- as.matrix(SLtdmIDF)
dfSLtdmIDF <- data.frame(words=rownames(mSLtdmIDF),mSLtdmIDF,freq=rowSums(mSLtdmIDF))
rownames(dfSLtdmIDF) <- NULL
highlander_freqIDF <- dfSLtdmIDF %>%
  select(words,freq) %>%
  top_n(20,freq) %>%
  arrange(-freq)
highlander_freqIDF

SLcorpus <- Corpus( VectorSource(D_4$txtL) )
SLcorpus


SLtdm <- TermDocumentMatrix(SLcorpus)
mSLtdm <- as.matrix(SLtdm)
dfSLtdm <- data.frame(words=rownames(mSLtdm),mSLtdm,freq=rowSums(mSLtdm))
rownames(dfSLtdm) <- NULL
freqTF <- dfSLtdm %>%
  select(words,freq) %>%
  top_n(n = 20,wt = freq) %>%
  arrange(-freq)

freqTF

SLtdmIDF <- weightTfIdf(SLtdm)
mSLtdmIDF <- as.matrix(SLtdmIDF)
dfSLtdmIDF <- data.frame(words=rownames(mSLtdmIDF),mSLtdmIDF,freq=rowSums(mSLtdmIDF))
rownames(dfSLtdmIDF) <- NULL
tucson_freqIDF <- dfSLtdmIDF %>%
  select(words,freq) %>%
  top_n(20,freq) %>%
  arrange(-freq)
tucson_freqIDF

}

# Same Process as before but using IDF weighting
SLtdmIDF <- weightTfIdf(SLtdm)
mSLtdmIDF <- as.matrix(SLtdmIDF)
dfSLtdmIDF <- data.frame(words=rownames(mSLtdmIDF),mSLtdmIDF,freq=rowSums(mSLtdmIDF))
rownames(dfSLtdmIDF) <- NULL
freqIDF <- dfSLtdmIDF %>%
  select(words,freq) %>%
  top_n(20,freq) %>%
  arrange(-freq)
freqIDF


# Sentiment Analysis 
source("./sentimentFunctions.R")
library(sentimentr)
library(RSentiment)
library(SentimentAnalysis)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)

syuz_sent <- get_nrc_sentiment(D_2$txtL.y, language = "english")
head(syuz_sent,5)

valence <- (syuz_sent[, 9]*-1) + syuz_sent[, 10]
table(valence)


syuz_polarity <- ifelse(valence<0,"negativo",ifelse(valence>0,"positivo","neutro"))
D_1<- cbind(D_1,syuz_polarity)
D_1_p<-D_1%>%filter(D_1$syuz_polarity=="positivo")
D_1_n<-D_1%>%filter(D_1$syuz_polarity=="negativo")
c_1_p <- Corpus( VectorSource(D_1_p) )
c_1_n <- Corpus( VectorSource(D_1_n) )

syuz_sent <- get_nrc_sentiment(D_2$txtL, language = "english")
head(syuz_sent,5)

valence <- (syuz_sent[, 9]*-1) + syuz_sent[, 10]
table(valence)
syuz_polarity <- ifelse(valence<0,"negativo",ifelse(valence>0,"positivo","neutro"))
D_2<- cbind(D_2,syuz_polarity)
D_2_p<-D_2%>%filter(D_2$syuz_polarity=="positivo")
D_2_n<-D_2%>%filter(D_2$syuz_polarity=="negativo")
c_2_p <- Corpus( VectorSource(D_2_p) )
c_2_n <- Corpus( VectorSource(D_2_n) )

syuz_sent <- get_nrc_sentiment(D_3$txtL, language = "english")
head(syuz_sent,5)

valence <- (syuz_sent[, 9]*-1) + syuz_sent[, 10]
table(valence)
syuz_polarity <- ifelse(valence<0,"negativo",ifelse(valence>0,"positivo","neutro"))
D_3<- cbind(D_3,syuz_polarity)
D_3_p<-D_3%>%filter(D_3$syuz_polarity=="positivo")
D_3_n<-D_3%>%filter(D_3$syuz_polarity=="negativo")
c_3_p <- Corpus( VectorSource(D_3_p) )
c_3_n <- Corpus( VectorSource(D_3_n) )

syuz_sent <- get_nrc_sentiment(D_4$txtL, language = "english")
head(syuz_sent,5)

valence <- (syuz_sent[, 9]*-1) + syuz_sent[, 10]
table(valence)
syuz_polarity <- ifelse(valence<0,"negativo",ifelse(valence>0,"positivo","neutro"))
D_4<- cbind(D_4,syuz_polarity)
D_4_p<-D_4%>%filter(D_4$syuz_polarity=="positivo")
D_4_n<-D_4%>%filter(D_4$syuz_polarity=="negativo")
c_4_p <- Corpus( VectorSource(D_4_p) )
c_4_n <- Corpus( VectorSource(D_4_n) )


# Co-occurance of terms
Lrav4<- dfLemm[1:20619,]
Lcrv<- dfLemm[20620:53650,]
Laccord<- dfLemm[53651:95126,]
Lcamry<- dfLemm[95127:116470,]
Lcivic<- dfLemm[116471:155969,]


Lcamry<- dfLemm[62563:82966,]

#freni
Lrav4f<-Lrav4
Lcrvf<-Lcrv
Laccordf<-Laccord
Lcamryf<-Lcamry
Lcivicf<-Lcivic

Lrav4f<-Lrav4f[Lrav4f$lemma == "brake", ]
Lrav4f

Lcrvf<-Lcrvf[Lcrvf$lemma == "brake", ]
Lcrvf

Laccordf<-Laccordf[Laccordf$lemma == "brake", ]
Laccordf

Lcamryf<-Lcamryf[Lcamryf$lemma == "brake", ]
Lcamryf

Lcivicf<-Lcivicf[Lcivicf$lemma == "brake", ]
Lcivicf

n<-Lrav4f$doc_id
n
x2<- as.data.frame(Lrav4f)

x2 <- as.data.frame(Lrav4[Lrav4$doc_id == "1",])
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "15",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "38",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "41",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "67",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "80",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "82",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "95",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "101",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "112",]))
x2 <- rbind( x2, as.data.frame(Lrav4[Lrav4$doc_id == "118",]))

x <- dfLemm[dfLemm$STOP == FALSE,]
x <- x[x$upos == "NOUN" | x$upos == "ADJ"|x$upos == "VERB",]
stats <- subset(x)
stats <- txt_freq(x = x$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))

barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")


## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("ADJ","VERB","NOUN")), 
                      term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))


stats<-as.data.frame(stats)
x2<- stats[(stats$term2=="price"),]
x2<- x2[order(x2$cooc,decreasing = TRUE),]

x2<-x2[1:30,c(1,3)]

barchart(x2$term1 ~ x2$cooc, data =x2, col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")


## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma, 
                      relevant = x$upos %in% c( "ADJ","NOUN"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma, 
                      relevant = x$upos %in% c("ADJ","VERB","NOUN"), skipgram =0)
#|x$lemma == "seat"
stats<-as.data.frame(stats)
x2<- stats[(stats$term2==""),]
x2<- x2[order(x2$cooc,decreasing = TRUE),]
x2<-x2[1:30,c(1,3)]

barchart(x2$term1 ~ x2$cooc, data =x2, col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")


{
library(igraph)
library(ggraph)
library(ggplot2)
wordnetwork <- head(stats, 20)
wordnetwork
wordnetwork <- graph_from_data_frame(wordnetwork)
wordnetwork

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  theme(legend.position = "none") +
  labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")
}

# Creating time series of the 4 brands 
s_storica <- cbind(prius_s$prius,focus_s$focus,tucson_s$tucson,highlander_s$highlander)

colnames(s_storica)<-c("prius","focus","tucson","highlander")
rownames(s_storica)<-c("2005","2006","2007","2008","2009","2010",
                       "2011","2012","2013","2014","2015","2016","2017")


s_storica_t<-as.vector(s_storica)
s_storica_t<-as.data.frame(t(s_storica_t))
s_storica_t

s_storica_t<-t(s_storica_t)


mean(z[c("prius_05","prius_07","prius_08","prius_09","prius_10","prius_11","prius_12",
         "prius_13",
         "focus_05","focus_06","focus_08","focus_09","focus_10","focus_11",
         "tucson_05","tucson_06","tucson_07","tucson_08","tucson_10","tucson_11","tucson_17",
         "highlander_05","highlander_06","highlander_07","highlander_08","highlander_09","highlander_11","highlander_12",
         "highlander_13","highlander_14","highlander_15","highlander_16","highlander_17")
])

mean(s_storica_t[c("tucson_12","tucson_13","tucson_08","tucson_17","tucson_14",
                   "tucson_09","tucson_11","focus_08","focus_09","focus_16",
                   "focus_07","focus_10","focus_15","focus_12","highlander_07",
                   "highlander_09"),])
var((s_storica_t[c("tucson_12","tucson_13","tucson_08","tucson_17","tucson_14",
                   "tucson_09","tucson_11","focus_08","focus_09","focus_16",
                   "focus_07","focus_10","focus_15","focus_12","highlander_07",
                   "highlander_09"),]))

mean(s_storica_t[c("prius_05","prius_17","prius_08","prius_15","prius_06","prius_16"),])

mean(s_storica_t[c("highlander_06" , "highlander_08" , "highlander_10" ,
                   "highlander_11", "highlander_12", "highlander_13",
                   "highlander_14","highlander_15", "highlander_16" ,"highlander_17",
                         "prius_07"  ,         "prius_09"  ,
                   "prius_10" ,     "prius_11" ,    "prius_12" ,    
                   "prius_13" ,     "prius_14"  ,    "focus_05"    ,  "focus_06" ,        
                           "focus_11" ,         
                   "focus_13"  ,    "focus_14"   ,        
                       "focus_17" ,  "tucson_05"   ,  "tucson_06"   ,  "tucson_07"    ,
                        "tucson_10"    
                  ),])

z<-s_storica_t[c("prius_05" ,     "prius_06"   ,   "prius_07"  ,    "prius_08"   ,   "prius_09"  ,
"prius_10"    ,  "prius_11"  ,    "prius_12"    , 
"prius_13"   ,   "prius_14"   ,   "prius_15"     , "prius_16"  ,    "prius_17"  ), ]  

var(z)
z<-s_storica_t[c("tucson_05" ,     "tucson_06"   ,   "tucson_07"  ,    "tucson_08"   ,   "tucson_09"  ,
                 "tucson_10"    ,  "tucson_11"  ,    "tucson_12"    , 
                 "tucson_13"   ,   "tucson_14"   ,   "tucson_15"     , "tucson_16"  ,    "tucson_17"  ), ]  

var(z)
mean(z)
